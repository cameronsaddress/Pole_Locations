import asyncio
import csv
import logging
import os
from contextlib import asynccontextmanager
from pathlib import Path
from typing import List, Optional

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
TRAIN_DIR = Path("/models/train_runs/yolov8l_custom2")
RESULTS_FILE = TRAIN_DIR / "results.csv"

# Mock Data Config
MOCK_ASSET_COUNT = 1250
HARRISBURG_CENTER = (40.26, -76.88)

class Asset(BaseModel):
    id: str
    lat: float
    lng: float
    status: str
    confidence: float
    detected_at: str
    issues: List[str] = []
    health_score: float = 1.0
    last_audit: Optional[str] = None
    financial_impact: float = 0.0

class OpsMetrics(BaseModel):
    total_assets: int
    grid_integrity: float
    daily_audit_count: int
    critical_anomalies: int
    preventative_savings: float

# Constants for Heuristics
ISSUE_TYPES = ["Leaning (Critical)", "Vegetation Encroachment", "Insulator Damage", "Rust/Corrosion"]
SAVINGS_MAP = {
    "Leaning (Critical)": 15000.0,      # Prevent pole collapse
    "Vegetation Encroachment": 5000.0,  # Prevent fire/outage
    "Insulator Damage": 2500.0,         # Prevent short circuit
    "Rust/Corrosion": 500.0             # Maintenance vs Replacement
}

def load_real_assets():
    """
    Load verified assets from the CSV generated by the pilot pipeline.
    PHASE 1 UPDATE: Inject Operational Intelligence (Heuristics)
    """
    assets = []
    csv_path = Path("/data/processed/verified_poles_multi_source.csv")
    
    # Fallback if running locally vs in docker (and data is in ./data)
    if not csv_path.exists():
        csv_path = Path("data/processed/verified_poles_multi_source.csv")

    if not csv_path.exists():
        print(f"WARNING: Real data file not found at {csv_path}. Returning empty list.")
        return []

    import random
    random.seed(42) # Deterministic for demos

    try:
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Map CSV status to UI status
                raw_class = row.get("classification", "unknown")
                if raw_class == "verified_good":
                    status = "Verified"
                elif raw_class == "in_question" or row.get("needs_review") == "True":
                    status = "Review"
                else:
                    status = "Missing"

                # Parse confidence and context
                try:
                    conf = float(row.get("total_confidence", 0.0))
                    road_dist = float(row.get("road_distance_m", 9999.0))
                except:
                    conf = 0.0
                    road_dist = 9999.0

                # AGGRESSIVE FILTER
                if status == "Review" and road_dist > 35.0:
                    continue

                # --- PHASE 1: DETERMINISTIC OP INTEL (NO MOCK) ---
                issues = []
                health = 1.0
                impact = 0.0
                
                # Logic: Real classification + Context (Road Dist) = Ops Level
                is_near_road = road_distance_m < 15.0
                
                if status == "verified_good" or status == "Verified":
                    status = "Verified"
                    health = 1.0
                    impact = 0.0
                else:
                    # It is "needs_review" or "in_question"
                    if is_near_road:
                        status = "Critical"
                        issues.append(f"Road Risk (Dist: {int(road_distance_m)}m)")
                        issues.append("Visual Anomaly")
                        health = 0.4
                        impact = 25000.0 # High liability cost
                    else:
                        status = "Flagged"
                        issues.append("Visual Anomaly")
                        health = 0.7
                        impact = 2000.0 # Inspection cost

                # Overwrite status for UI if critical
                ui_status = status

                assets.append(Asset(
                    id=row.get("pole_id", "UNKNOWN"),
                    lat=float(row.get("lat", 0.0)),
                    lng=float(row.get("lon", 0.0)),
                    status=ui_status,
                    confidence=conf,
                    detected_at=row.get("inspection_date") or datetime.now().isoformat(),
                    issues=issues,
                    health_score=round(health, 2),
                    financial_impact=impact,
                    last_audit=datetime.now().isoformat() # Simulate live
                ))
        print(f"Loaded {len(assets)} real assets from {csv_path}")
    except Exception as e:
        print(f"Error loading assets: {e}")
        
    return assets

CACHED_ASSETS = load_real_assets()

# --- OPERATIONAL QUERY HELPERS ---
def get_ops_metrics_snapshot():
    total = len(CACHED_ASSETS)
    if total == 0: return OpsMetrics(total_assets=0, grid_integrity=1.0, daily_audit_count=0, critical_anomalies=0, preventative_savings=0.0)
    
    anomalies = [a for a in CACHED_ASSETS if a.health_score < 1.0]
    savings = sum(a.financial_impact for a in anomalies)
    
    # Grid Integrity: Average health score of the entire grid
    avg_health = sum(a.health_score for a in CACHED_ASSETS) / total
    
    return OpsMetrics(
        total_assets=total,
        grid_integrity=round(avg_health * 100, 2), # e.g. 98.5
        daily_audit_count=len([a for a in CACHED_ASSETS if a.status == "Verified"]), # Mock 'Daily'
        critical_anomalies=len([a for a in CACHED_ASSETS if "Critical" in a.status or a.health_score < 0.5]),
        preventative_savings=savings
    )

@app.get("/api/v2/assets", response_model=List[Asset])
async def get_assets(
    min_lat: Optional[float] = None,
    max_lat: Optional[float] = None,
    min_lng: Optional[float] = None,
    max_lng: Optional[float] = None,
    status: Optional[str] = None
):
    """
    Get assets, filtered by bounds AND status.
    """
    if min_lat is None: return []

    COUNTY_BOUNDS = [
        [40.12, -77.05, 40.67, -76.55], # Dauphin
        [39.71, -77.15, 40.25, -76.40], # York
        [40.0, -77.60, 40.35, -76.85],  # Cumberland
        [39.72, -77.47, 40.0, -77.06],  # Adams
        [40.23, -76.65, 40.58, -76.28],  # Lebanon
    ]

    def is_in_whitelist(lat, lng):
        for b in COUNTY_BOUNDS:
            if b[0] <= lat <= b[2] and b[1] <= lng <= b[3]: return True
        return False

    # pre-filter by status if requested (e.g. "Critical")
    source_list = CACHED_ASSETS
    if status == "critical":
        source_list = [a for a in CACHED_ASSETS if a.health_score < 0.7 or a.status in ["Critical", "Flagged"]]

    filtered = [
        a for a in source_list
        if min_lat <= a.lat <= max_lat and min_lng <= a.lng <= max_lng and is_in_whitelist(a.lat, a.lng)
    ]
    
    if len(filtered) > 5000:
        def get_priority(asset):
            if asset.status == "Critical": return 5
            if asset.status == "Flagged": return 4
            if asset.status == "Verified": return 3
            return 1
        filtered.sort(key=lambda x: (get_priority(x), x.confidence), reverse=True)
        return filtered[:5000]
        
    return filtered

# --- NEW OPS CENTER ENDPOINTS ---

@app.get("/api/v2/ops/metrics", response_model=OpsMetrics)
async def get_ops_metrics():
    return get_ops_metrics_snapshot()

@app.get("/api/v2/ops/feed/anomalies", response_model=List[Asset])
async def get_anomaly_feed(limit: int = 50):
    """Returns list of most recent detected anomalies for the Satellite Feed"""
    # Sort by lowest health first
    anomalies = [a for a in CACHED_ASSETS if a.health_score < 1.0]
    anomalies.sort(key=lambda x: x.health_score)
    return anomalies[:limit]

@app.get("/api/v2/ops/audit-log")
async def get_audit_log(limit: int = 20):
    """Returns a stream of recent audits (Mixed verified + anomalies)"""
    # Just take the first N for now since we don't have real timestamps updating live
    # Ideally shuffle or rotate to make it look "live"
    import random
    sample = random.sample(CACHED_ASSETS, min(len(CACHED_ASSETS), limit))
    return sample

@app.websocket("/ws/training")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            stats = get_latest_stats()
            await websocket.send_json(stats.model_dump())
            await asyncio.sleep(2) # Stream updates every 2s
    except WebSocketDisconnect:
        manager.disconnect(websocket)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
